{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":34377,"databundleVersionId":3220602,"sourceType":"competition"}],"dockerImageVersionId":30839,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Step 1: Load the data\ntrain = pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')\n\n# Step 2: Drop the 'Name' column (which is non-numeric and not needed)\ntrain = train.drop(columns=['Name'])\ntest = test.drop(columns=['Name'])\n\n# Step 3: Handle missing values\n# Separate numeric and categorical columns (after dropping 'Name')\nnumeric_cols = train.select_dtypes(include=['float64', 'int64']).columns\ncategorical_cols = train.select_dtypes(include=['object']).columns\n\n# For numeric columns, use mean strategy\nnumeric_imputer = SimpleImputer(strategy='mean')\ntrain[numeric_cols] = numeric_imputer.fit_transform(train[numeric_cols])\ntest[numeric_cols] = numeric_imputer.transform(test[numeric_cols])\n\n# For categorical columns, use most_frequent strategy\ncategorical_imputer = SimpleImputer(strategy='most_frequent')\ntrain[categorical_cols] = categorical_imputer.fit_transform(train[categorical_cols])\ntest[categorical_cols] = categorical_imputer.transform(test[categorical_cols])\n\n# Step 4: Encode categorical columns\ncat_features = ['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP']\nle = LabelEncoder()\n\n# Fit and transform on train\nfor col in cat_features:\n    train[col] = le.fit_transform(train[col].astype(str))  # Fit and transform on training data\n\n    # Handle unseen categories during transformation by adding missing categories to the training set\n    missing_categories = np.setdiff1d(test[col].unique(), le.classes_)\n    le.classes_ = np.concatenate([le.classes_, missing_categories])\n\n    test[col] = le.transform(test[col].astype(str))  # Transform on test data using the same fit\n\n# Step 5: Split into features and target\nX = train.drop(columns=['Transported', 'PassengerId'])\ny = train['Transported']\n\n# Step 6: Train-test split (using a validation set)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Step 7: Initialize XGBoost Model\nmodel = XGBClassifier(\n    n_estimators=1000,  # Number of trees\n    learning_rate=0.05,  # Learning rate\n    max_depth=6,  # Depth of each tree\n    subsample=0.8,  # Proportion of samples used per tree\n    colsample_bytree=0.8,  # Proportion of features used per tree\n    random_state=42,\n    enable_categorical=True  # Enable categorical handling\n)\n\n# Step 8: Train the model\nmodel.fit(X_train, y_train)\n\n# Step 9: Model Evaluation\ny_pred = model.predict(X_val)\naccuracy = accuracy_score(y_val, y_pred)\nprint(f'Validation Accuracy: {accuracy:.4f}')\n\n# Step 10: Predictions on the test set\ny_test_pred = model.predict(test.drop(columns=['PassengerId']))\n\n# Step 11: Prepare the submission file\nsubmission = pd.DataFrame({\n    'PassengerId': test['PassengerId'],\n    'Transported': y_test_pred\n})\n\n# Step 12: Save the submission to CSV\nsubmission.to_csv('submission.csv', index=False)\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinerRegression\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n\n#Load the dataset\n\ndf = pd.read_csv(\"train.csv\")\ndf = df[['GrLivArea','BedroomAbGr','FullBath','saleprice']]\ndf['TotalBath'] = df['FullBath'] + (0.5 * df['HalfBath'])\ndf = df[['GrLivArea','BedroomAbGl','TotalBath','SalePrice']]\ndf.dropna(inplace=True)\n\n#Define Features and Target\n\nx = df[['GrLivArea','BedroomAbGr','TotalBath','SalePrice']]\ny = df['saleprice']\n\n#Split the Data\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n\n#Train the Linear Regression Model\n\nmodel = LinearRegression()\nmodel.fit(x_train,Y_train)\n\n#Make Predictions\n\ny_pred = model.predict(x_test)\n\n#Evaluated the model\n\nmae = mean_absolute_error(y_test,y_pred)\nmse = mean_squared_error(y_test,y_pred)\nrmse = (f\"Root Mean Squared Error: {rmse}\")\nr2 = r2_score(y_test,y_pred)\n\nprint(f\"Mean Absolute Error: {mae}\")\nprint(f\"Mean Squared Error: {mse}\")\nprint(f\"R^2 score: {r2}\")\n\n#Visualizing the predictions\n\nplt.scatter(y_test,y_pred,alpha=0.5)\nplt.xlabel(\"Actual Prices\")\nplt.ylabel(\"predicted prices\")\nplt.title(\"Actual vs Predicted House Prices\")\nplt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}